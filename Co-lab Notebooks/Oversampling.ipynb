{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Ddg55LkQX_oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accessing data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o1LVtW2yBUE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWKDXxCJX9Ow"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/AIP Team 5/new_data.csv')\n",
        "\n",
        "# first 5 samples from dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting comments into lowercase\n",
        "data['processed_text'] = data['processed_text'].str.lower()"
      ],
      "metadata": {
        "id": "oIAoI-RpD75e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating X and y for train and test\n",
        "X= data['processed_text']\n",
        "y= data['label']"
      ],
      "metadata": {
        "id": "kvZMFZ4ppys8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "metadata": {
        "id": "ozvbC_KkqPuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)"
      ],
      "metadata": {
        "id": "l8V-UMMFqWRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(y_train)"
      ],
      "metadata": {
        "id": "0F71JJ_pqj2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(X_train)"
      ],
      "metadata": {
        "id": "Nq8acvQ6qq2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tf = vectorizer.transform(X_train)"
      ],
      "metadata": {
        "id": "GxhuanoYquYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overSampler = RandomOverSampler(sampling_strategy=0.5)\n",
        "X_train_os, y_train_os = overSampler.fit_resample(X_train_tf, y_train)"
      ],
      "metadata": {
        "id": "D0a1bu3uqxDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorizer.inverse_transform(X_train_os)"
      ],
      "metadata": {
        "id": "XCV8h7SruuLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train),len(y_train_os)"
      ],
      "metadata": {
        "id": "M4XldgN5xVE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the embedding layer using pre-trained weights, taken from Glove\n",
        "embedding_dict = {}\n",
        "\n",
        "glove_file_path = r\"/content/drive/MyDrive/Sem 3/Deep Learning/glove.6B.100d.txt\"\n",
        "with open(glove_file_path,'r') as f:\n",
        "    for line in f:\n",
        "\n",
        "        # Every lines contains word and then its embedding\n",
        "        # spliting the line\n",
        "        values = line.split()\n",
        "\n",
        "        # first token will be the word\n",
        "        word = values[0]\n",
        "\n",
        "        # rest of the tokens are the embedding values of that word\n",
        "        vectors = np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word] = vectors\n",
        "        \n",
        "f.close()"
      ],
      "metadata": {
        "id": "17h0JFsjzU-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = []\n",
        "for xt in X_train:\n",
        "  temp.append(' '.join(xt))\n"
      ],
      "metadata": {
        "id": "TqeGgXz39mxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = temp"
      ],
      "metadata": {
        "id": "HY_wz3LOCBH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[15]"
      ],
      "metadata": {
        "id": "08o8O7OZAmG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bvi3V5Dl8XwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# encoding the words to numbers\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoding = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_encoding = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_review_length = 70\n",
        "\n",
        "# using padding to make every review of equal size\n",
        "X_train = pad_sequences(X_train_encoding, maxlen=max_review_length,padding='post')\n",
        "X_test = pad_sequences(X_test_encoding, maxlen=max_review_length,padding='post')\n",
        "\n",
        "words_to_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "L92evE1pzv-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length of our vocab\n",
        "vocab_len = len(words_to_index)+1\n",
        "\n",
        "# defining the numpy matrix to store the encodings\n",
        "emb_matrix = np.zeros((vocab_len, 100))\n",
        "\n",
        "not_list = []\n",
        "for word, index in words_to_index.items():\n",
        "  embedding_vector = embedding_dict.get(word)\n",
        "\n",
        "  # if word is in the embedding dictionary else discard it\n",
        "  if embedding_vector is not None:\n",
        "    emb_matrix[index, :] = embedding_vector\n",
        "  else:\n",
        "    not_list.append(index)\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_len, output_dim=100, input_length=max_review_length, weights = [emb_matrix], trainable=True)"
      ],
      "metadata": {
        "id": "h8J_Vlpu0DeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "embedding_dim = 16\n",
        "lstm_dim = 32\n",
        "dense_dim = 24\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim,return_sequences=True,dropout=0.2)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim,return_sequences=True,dropout=0.2)),\n",
        "    tf.keras.layers.LSTM(lstm_dim,dropout=0.2),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9C0ZwZjCGBhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Train the model\n",
        "history_lstm = model.fit(X_train,y_train_os, epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "rZg5Jns9GDVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X_test)"
      ],
      "metadata": {
        "id": "Gdy-VYbxGGGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (prediction > 0.5)"
      ],
      "metadata": {
        "id": "J6IYsK7uuQ-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix,recall_score,precision_score\n",
        "\n",
        "print(\"Accuracy of the model : \", accuracy_score(y_pred, y_test))\n",
        "print('F1-score: ', f1_score(y_pred, y_test))\n",
        "print('Confusion matrix:')\n",
        "confusion_matrix(y_test,y_pred)"
      ],
      "metadata": {
        "id": "NPjq4mXut0Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zy8LY0Eqt6PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X_train)\n",
        "y_pred = (prediction > 0.5)"
      ],
      "metadata": {
        "id": "67Aj_aE7t6SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix,recall_score,precision_score\n",
        "\n",
        "print(\"Accuracy of the model : \", accuracy_score(y_pred, y_train_os))\n",
        "print('F1-score: ', f1_score(y_pred, y_train_os))\n",
        "print('Confusion matrix:')\n",
        "confusion_matrix(y_train_os,y_pred)"
      ],
      "metadata": {
        "id": "Owk5PlDYt6Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5weSKByFt6YE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}